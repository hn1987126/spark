##########################################################################
########## 知识、技巧
##########################################################################
scp 的时候，在当前目录下，如果拷过去的对方也是在那个目录下，可以用 $PWD
scp a.txt s1:$pwd

### Copy Reference 是复制包名+类名
### 显示目录大小   du -h ~/.m2




##########################################################################
########## scala
##########################################################################
val   是value，值，定义的是常量，不可改变
var   是variable，变量的意思，可以改变
def   定义的是方法
函数不需要def，直接  val fun1 =  (v1:Int)=>v1*100，定义的函数可以赋值给变量，这样就可以把这变量当参数进行传递，去再调别的方法。
map   的意思是将 序列，集合，数组等里面的每一个值进行拿出来处理，这就可以  如对a这个集合处理，a.map(fun1)  这样调用，结果就是对a的每个元素都乘100
想实现上面的效果，也可以这样执行：a.map(_*100)，这就是scala里神奇的下划线_，它就是点位符。

函数变形：
val fun1 =  (v1:Int)=>v1*100
val fun12 =  (v1:Int, v2:String) => {v1+v2}
var fun2 : Int=>String = { x => x.toString}
如果两个参数咋接收？
var fun3 : (Int,String) => (String, Int) = {
    (x, y) => {(y+1, x+1)}
}
空返回值的方法
def testa(str :String) : Unit = { xxx }或者
def testa(str :String) = { xxx }   空返回，实际上返回的是 ()
## 把方法变为函数：：[神奇的下划线]
def mm1(v1:Int, v2:Int) : Int = {v1+v2}
val ff2 = mm1 _     与m1功能完全相同的函数，可以当变量传给别的方法。


#### 柯里化的两种表现形式：
def m(x : Int)(y: Int) = x*y
def m1(x : Int) = (y : Int) => x*y

#### def 这种方法，可以直接调用他，但是想把他当函数一样做为参数传递，需要在后面加上 _，或者不带参数这种，系统会隐式的转换为函数。

#### 泛型、隐式转换
最大边界 ( <: )(UpperBound)，定义传过来的泛型的最大的边界，必须要小于某个层级的类
最大边界 ( >: )(LowerBound)，定义传过来的泛型的最小的边界，必须要大于某个层级的类
视图界定( <% )(ViewBound)，必须传进去一个隐式转换的函数
上下文界定(:)，必须传进去一个隐式转换的值

####
关于compareTo比较方法，前面大于后面，返回1，代表正序排序(由低到高)。前面小于后面，返回-1，代表倒序排序(由高到低)。
前面减后面，代表降序。后面减前面代表正序。




#############################################################################
####################### spark
#############################################################################
下载地址：
https://archive.apache.org/dist/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz

解压：
tar -vxzf /home/hadoop/soft/spark-1.6.1-bin-hadoop2.6.tgz -C /export/servers/

## 安装
cd /export/servers/
mv spark-1.6.1-bin-hadoop2.6 spark
cd spark/conf
mv spark-env.sh.template spark-env.sh

cat >> spark-env.sh << EOF
#新加的配置
export JAVA_HOME=/usr/local/java
export SPARK_MASTER_IP=s1
export SPARK_MASTER_PORT=7077
EOF

# 配置slave节点
mv slaves.template slaves
cat >> slaves << EOF

s2
s3
s4
EOF


#### 将配置好的Spark拷贝到其他节点上
cd /export/servers
scp -r spark s2:$PWD

### 在s1上添加环境变量：
su - root
vi /etc/profile
#加入：
export SPARK_HOME=/export/servers/spark
export PATH=$PATH:$SPARK_HOME/bin

source /etc/profile
su - hadoop
source /etc/profile

###
Spark集群配置完毕，目前是1个Master，2个Work（s1上既有master也有work），在 s1 上启动Spark集群
/export/servers/spark/sbin/start-all.sh
在这之前需要启动hdfs    start-dfs.sh

###
启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：http://s1:8080/


### zookeeper  这个先不配置
到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：
Spark集群规划：node1，node2是Master；node3，node4，node5是Worker
安装配置zk集群，并启动zk集群
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark"
1.在node1节点上修改slaves配置文件内容指定worker节点
2.在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master


### 执行第一个spark程序, 最后的100是指经过100次迭代
spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://s1:7077 \
--executor-memory 1G \
--total-executor-cores 1 \
/export/servers/spark/lib/spark-examples-1.6.1-hadoop2.6.0.jar \
100


### 启动spark shell
spark-shell --master spark://s1:7077
或者指定后面的参数
spark-shell --master spark://s1:7077 --executor-memory 512m --total-executor-cores 1


### 使用spark-submit命令提交Spark应用（注意参数的顺序）
spark-submit \
--class cn.jhsoft.spark.wordcount.WordCount \
--master spark://s1:7077 \
--executor-memory 1G
--total-executor-cores 2
/home/hadoop/spark-1.0.jar \
hdfs://s1:9000/wordcount/input/a.txt \
hdfs://s1:9000/out2


###在spark shell中编写WordCount程序
#首先启动hdfs
start-dfs.sh

#向hdfs上传一个文件到 hdfs://s1:9000/words.txt
#在spark shell中用scala语言编写spark程序
#结果输出到控制台
sc.textFile("hdfs://s1:9000/wordcount/input/a.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
sc.textFile("hdfs://s1:9000/wordcount/input/a.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2, false).collect
#结果放入hdfs，这个如果有多个文件(如果读的hdfs是目录的话)，会写入多个结果
sc.textFile("hdfs://s1:9000/wordcount/input/a.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2, false).saveAsTextFile("hdfs://s1:9000/out")
#把结果写入一个文件，也就是启动一个mapReduces，设置 reduceByKey(_+_, 1)
sc.textFile("hdfs://s1:9000/wordcount/input/a.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile("hdfs://s1:9000/out")


#### spark算子(就是函数的意思) 分两类
一类叫 Transformation  (转换)，如有map,textFile等
一类叫 Action          (动作)，常用的有如 take是指取多少条，count是得到数量，collect是把结果收集起来可以赋值给一个变量,打印到屏幕

Transformation延迟执行，它只会记录元数据信息，当计算任务触发Action时才会真正开始计算。

### RDD[分布式的数据集]   的创建有两类：
1是从文件textFile，hdfs，RDD里没有真正要计算的数据，只记录一下元数据
2是通过scala集合 sc.parallelize  或数组以并行化的方式创建RDD，引用原来的集合
val rdd1 = sc.parallelize(Array(1,2,3,4,5,6))
查看rdd的分区数量
rdd1.partitions.length
可以指定rdd的分区数
val rdd2 = sc.parallelize(Array(1,2,3,4,5,6), 5)
#对rdd1进行每个元素
rdd1.map(_*10).sortBy(x=>x,false).collect

###################################################################################################
#spark Transformation
val rdd1 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10))
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x,true)
val rdd3 = rdd2.filter(_>10)
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x+"",true)
val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=>x.toString,true)
val rdd4 = sc.parallelize(Array("a b c", "d e f", "h i j"))
rdd4.flatMap(_.split(' ')).collect
val rdd5 = sc.parallelize(List(List("a b c", "a b b"),List("e f g", "a f g"), List("h i j", "a a b")))

List("a b c", "a b b") =List("a","b",))

rdd5.flatMap(_.flatMap(_.split(" "))).collect

#union并集
val rdd6 = sc.parallelize(List(5,6,4,7))
val rdd7 = sc.parallelize(List(1,2,3,4))
val rdd8 = rdd6.union(rdd7)
rdd8.distinct.sortBy(x=>x).collect

#intersection交集
val rdd9 = rdd6.intersection(rdd7)
val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3)))
val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7), ("tom", 2)))

#join  k,v形式的才能用join，这需要用上两行的 rdd1和rdd2
val rdd3 = rdd1.join(rdd2)
val rdd3 = rdd1.leftOuterJoin(rdd2)
val rdd3 = rdd1.rightOuterJoin(rdd2)

#groupByKey
val rdd3 = rdd1 union rdd2
rdd3.groupByKey
rdd3.groupByKey.map(x=>(x._1,x._2.sum))
rdd3.groupByKey.mapValues(_.sum)

#WordCount
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).groupByKey.map(t=>(t._1, t._2.sum)).collect

#cogroup
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)))
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))
val rdd3 = rdd1.cogroup(rdd2)
val rdd4 = rdd3.map(t=>(t._1, t._2._1.sum + t._2._2.sum))

#cartesian 笛卡尔积
val rdd1 = sc.parallelize(List("tom", "jerry"))
val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"))
val rdd3 = rdd1.cartesian(rdd2)

###################################################################################################
#spark action
val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)
#collect
rdd1.collect
#reduce
val rdd2 = rdd1.reduce(_+_)
#count
rdd1.count
#top 排序取最大的
rdd1.top(2)
#take 取前2个，不排序就只是取前2个
rdd1.take(2)
#first 与take(1)相同，只不过take(1)是数组，这个first是直接取出了元素
rdd1.first
#takeOrdered
rdd1.takeOrdered(3)


###################################################################################################
RDD 高级算子
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
map是对每个元素操作, mapPartitions是对其中的每个partition操作
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码
val func = (index: Int, iter: Iterator[(Int)]) => {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func).collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
aggregate

def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func1).collect    #这个是看每个分区里有什么，有哪些元素
###是action操作, 第一个参数是初始值, 第二个参数:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]
###0 + (0+1+2+3+4   +   0+5+6+7+8+9)
rdd1.aggregate(0)(_+_, _+_)
### 比较不同分区的各自的最大值，把他们的最大值相加，如果有初始值，那就与各分区的相加，再最后与汇总的结果相加
rdd1.aggregate(0)(math.max(_, _), _ + _)
### 比较不同分区的最大值，再在各分区里取最大值。
rdd1.aggregate(0)(math.max(_, _), math.max(_,_))
### 这个是10分别和第一分区的比，10分别和第二分区的比，他们的最大值，再跟10比。意思是说10会与各分区，还有最后汇总的结果都有关系 。
rdd1.aggregate(10)(math.max(_, _), math.max(_,_))
###5和1比, 得5再和234比得5 --> 5和6789比,得9 --> 5 + (5+9)
rdd1.aggregate(5)(math.max(_, _), _ + _)
val arr = Array(1,2,3)
#这是两两比较 最大值，得出最终的最大值
arr.reduce(math.max(_,_))


val rdd2 = sc.parallelize(List("a","b","c","d","e","f"),2)
def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
rdd2.aggregate("")(_ + _, _ + _)
rdd2.aggregate("|")(_ + _, _ + _)     结果为||abc|def    每个分区前面有个|，最后的结果前面再有个|

val rdd3 = sc.parallelize(List("12","23","345","4567"),2)
rdd3.aggregate("")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)
# 上面的结果是对每个分区里的元素，字符串长度比较出最大值，转为字符串，第一分区为2，第二分区为4，汇聚的方式是连接，得24，但也有可能并行计算分区2先完成，得4和2相连，得42

val rdd4 = sc.parallelize(List("12","23","345",""),2)
rdd4.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)
# 结果是 01或10，之前还是没有理解这种方式，这是把初始值先和第一个数比，结果如谁最小，再跟第二个数比。这样来循环的。
# 第一个分区来如下两轮
# math.min("".length, "12".length)  结果是 0，因为是求长度的最小值。下一轮是用这一轮的结果，代入进去跟下一个数比较
# math.min("0".length, "23".length)  结果是1。因为是求min(1,2)得到1

val rdd5 = sc.parallelize(List("12","23","","345"),2)
rdd5.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
aggregateByKey

val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)
# 这个func2和下面的 mapPartitionsWithIndex 主要是为了看下，哪个分区里有些什么元素
def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
pairRDD.mapPartitionsWithIndex(func2).collect
# 局部[每个分区]根据key进行操作，先对相同的key求和，再汇总对结果求和。这个其实和 pairRDD.reduceByKey(_+_).collect 功能相同
pairRDD.aggregateByKey(0)(_+_, _ + _).collect
# 局部根据key进行操作，再全部根据key进行操作，结果为 Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))
pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
检查点
checkpoint
sc.setCheckpointDir("hdfs://s1:9000/ck")
val rdd = sc.textFile("hdfs://s1:9000/wordcount/input").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
rdd.checkpoint
rdd.isCheckpointed
rdd.count
rdd.isCheckpointed
rdd.getCheckpointFile

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
重新分区，repartition底层其实就是调用coalesce方法，只不过第二个参数是传的true，代表要立即分区，如果coalesce第二个参数是传false，那其实没啥用，并没有分区
coalesce, repartition
val rdd1 = sc.parallelize(1 to 10, 10)
val rdd2 = rdd1.coalesce(2, false)
rdd2.partitions.length
val rdd3 = rdd1.repartition(2)    相当于 rdd1.coalesce(2, true)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
把结果转为Map
collectAsMap : Map(b -> 2, a -> 1)
val rdd = sc.parallelize(List(("a", 1), ("b", 2)))
rdd.collectAsMap

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
combineByKey : 和reduceByKey是相同的效果
###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算
###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)-->(hello(1,1),good(1))-->x就相当于hello的第一个1, good中的1
val rdd1 = sc.textFile("hdfs://s1:9000/wordcount/input/").flatMap(_.split(" ")).map((_, 1))
val rdd2 = rdd1.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
# 上面的这个 x=>x 是相当于根据key分组出来的第一个参数，如果 x=> x+10相当于初始值为10，a+b是代表用x的值再跟第二个元素值比较，m+n是指结果汇总的累计
rdd1.collect
rdd2.collect

###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加1个10，每个分区(block)只加一次哟
val rdd3 = rdd1.combineByKey(x => x + 10, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
rdd3.collect


val rdd4 = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)
# 拉链  结果为Array((1,dog), (1,cat), (2,gnu), (2,salmon), (2,rabbit), (1,turkey), (2,wolf), (2,bear), (2,bee))
val rdd6 = rdd5.zip(rdd4)
val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) => x :+ y, (m: List[String], n: List[String]) => m ++ n)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
按key或按value进行统计
countByKey

val rdd1 = sc.parallelize(List(("a", 1), ("b", 2), ("b", 2), ("c", 2), ("c", 1)))
rdd1.countByKey
rdd1.countByValue

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
过滤时给个范围，如rdd1.filterByRange("b", "d")会把，b、c、d的都过滤出来。是征对里面的key的。
filterByRange

val rdd1 = sc.parallelize(List(("e", 5), ("c", 3), ("d", 4), ("c", 2), ("a", 1)))
val rdd2 = rdd1.filterByRange("b", "d")
rdd2.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
这是把数组的value压平。再和key进行两两组合
flatMapValues  :  Array((a,1), (a,2), (b,3), (b,4))
val rdd3 = sc.parallelize(List(("a", "1 2"), ("b", "3 4")))
val rdd4 = rdd3.flatMapValues(_.split(" "))
rdd4.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
foldByKey

val rdd1 = sc.parallelize(List("dog", "wolf", "cat", "bear"), 2)
val rdd2 = rdd1.map(x => (x.length, x))
val rdd3 = rdd2.foldByKey("")(_+_)
#结果为  Map((3,dogcat), (4,wolfbear))

val rdd = sc.textFile("hdfs://node-1.itcast.cn:9000/wc").flatMap(_.split(" ")).map((_, 1))
rdd.foldByKey(0)(_+_)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
foreachPartition   是按分区来循环的。不产生新的数组
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
rdd1.foreachPartition(x => println(x.reduce(_ + _)))

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
keyBy : 以传入的参数做key
val rdd1 = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val rdd2 = rdd1.keyBy(_.length)    结果是 Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))
rdd2.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
把里面的key取出来生成一个一维数组。values是把value取出来生成一维数组
keys values
val rdd1 = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val rdd2 = rdd1.map(x => (x.length, x))
rdd2.keys.collect
rdd2.values.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------

#### lineage 血统：记录每个RDD的元数据信息和转换行为，当RDD的部分数据丢失时，它可以根据这些信息来重新运算和恢复数据的分区。

########  (在spark命令行里执行的一条命令，如 val rdd = sc.textFile("hdfs://s1:9000/wordcount/input/a.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)   )
用 rdd.toDebugString  可以把各步骤的依赖关系打印出来，也就是比如wordcount，要做很多步，如hdfs,flatMap,map,reduceByKey等。
rdd.dependencies  可以看当前状态，当前是谁，一般是 org.apache.spark.shuffledRDD

RDD和RDD（即多条命令行里的语句之间）也是有依赖关系的
RDD是分布式的数据集，里面有HDFS，Mysql，还有集合等。

对于wordcount程序:
textFile方法会产生两个RDD，1、HadoopRDD，2、MapPartitionRDD【调用了spark的map方法，把第一步hadoop返回的偏移量去掉了(hadoop返回了，偏移量和行内容)】
flatMap 方法会产生一个RDD   MapPartitionRDD
map 方法会产生一个RDD   MapPartitionRDD
reduceByKey 方法会产生一个RDD   ShuffledRDD  [下游到上游来拉取数据的过程，那就要等上游的数据处理完才能拉]
saveToTextFile  方法产生一个RDD    MapPartitionRDD

### jps可知，启动spark-shell命令行，它也是一个app任务，在worker机器上会有这个进程  CoarseGrainedExecutorBackend  也就是Executor
在 master那台机器上，会有SparkSubmit 进程。也就是 Driver

### 远程提交，本地的程序，提交到远程集群里，本地调试。
debug调试后，可以在本地电脑上 jps看到，会多了一个 WordCountRemote(如运行的是WordCountRemote这个类)。这个相当于是 Driver

### spark的缓存机制，在之前的方法之后加个   cache()  就代表缓存起来，他底层是调persist()方法。 其中StorageLevel级别为-> MEMORY_ONLY只放内存里，MEMORY_ONLY_2是指内存中放2份，一份在本机，一份在别的机器
如：val rdd1 = sc.textFile("hdfs:xxx")
rdd1.cache()### 释放缓存   rdd1.unpersist(true)

###### 设置还原点：checkpoint。这个适合于，某个计算的结果很重要，他很慢，而且他下面有很多其他的计算要依赖他。这时适合把他放入还原点checkpoint。
###### 设置了checkpoint，并且有Action执行的时候，会把这之前依赖的父动作给删掉，将来有意外要恢复的时候也是从checkpoint里进行恢复，而不从血统轨迹里恢复。
sc.setCheckpointDir("hdfs:s1:9000/ck")
val rdd1 = rdd2.map(x=>x._2, 1).checkpoint   // 想用checkpoint，必须在此之前先执行  setCheckpointDir
rdd.count   这时是Action，会把checkpoint提交，此时hdfs目录的根目录下，ck目录下就有数据了。在上一句执行checkpoint的时候，只是会建个ck目录，里面没有数据。
建议先  cache()，再checkpoint, 再collect等Action。这样的话，checkpoint就可以从内存中取了。

##### RDD  有方向无闭环 DAG，DAG最后的边界，是Action的时候。
DAG 的名字叫 有向无环图。

##### pepiline 流水线
ShuffledRDD 之前的是一个流水线，是上游，是窄依赖   也就是wordcount里的 reduceByKey之前的动作。
reduceByKey和他之后的saveAsTextFile，他们两又是一条流水线。

##### 流程：
Driver提交submit任务，Master根据提交任务时指定的参数如内存cpu核数等决定要用哪几台 worker来执行任务。确定了以后，就没有Master什么 事了。
然后Driver就和Worker建立了通信，Driver会把任务给Worker，Worker处理完后给Driver反馈。
如果是处理完后需要写数据如把处理结果写入mysql或redis保存，最好是由Worker直接操作，因为是有多台Worker并发来操作，速度更快。而且 省去了给Driver再反馈的网络开销。

##### RDD的依赖关系：
RDD和他的父RDD有依赖关系，分 宽依赖（wide dependency）和 窄依赖（narrow dependency）。
窄依赖 如map、filter、union等是在本分区进行
join大多情况下是宽依赖，只有一种情况是窄依赖（先进行groupBy后）。窄依赖相当于独生子女，宽依赖相当于多生子女。
宽依赖 就是可以切一刀，它之前是一阶段，之后是一阶段。

##### Stage是阶段的意思。

##### spartSQL
在spark Shell终端里进行测试：
sqlContext
val rdd=sc.textFile("hdfs://s1:9000/wordcount/input/1.txt").map(_.split(","))
case class Person(id:Long, name:String, age:Int)
val personRDD = rdd.map(x=>Person(x(0).toLong, x(1), x(2).toInt))
val df = personRDD.toDF

## DSL风格
df.show    // 这是Action
df.select("id", "name").show   //select是Transfotion
df.filter(col("age")>=18).show

## SQL风格
df.registerTempTable("t_person")
sqlcontext.sql("select * from t_person order by age desc limit 2").show
sqlcontext.sql("desc t_person").show

## 编程需要引依赖
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.10</artifactId>
    <version>${spark.version}</version>
</dependency>

编程时需要导入隐式转换，不然无法使用 rdd.toDF
import sqlContext.implicits._

## 结果转为json存入到hdfs
sqlcontext.sql("select * from t_person order by age desc limit 2").write.json("hdfs://s1:9000/json")

## 介绍下 Parquest File这是一种经压缩的对数据进行列式存储的。比ORC还有优势。

## 保存结果 save和write都可以用，save的话，在后面的参数上加 json
result.save("hdfs://s1:9000/json")   // 如果不指定类型，则是 Parquest File 这种压缩形式的列式存储
result.save("hdfs://s1:9000/json", "json")

#以JSON文件格式覆写HDFS上的JSON文件
import org.apache.spark.sql.SaveMode._
result.save("hdfs://s1:9000/json", "json" , Overwrite)

## 重新加载以前的处理结果（可选）
-- 以json的形式来读取数据。读进来以后便是 DataFrame，直接就把json里的key-v映射到字段上了
sqlContext.load("hdfs://s1:9000/json", "json")






































